{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Webscrapping from Sephora website"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: Scrapy in /opt/anaconda3/lib/python3.7/site-packages (1.6.0)\n",
      "Requirement already satisfied: Twisted>=13.1.0 in /opt/anaconda3/lib/python3.7/site-packages (from Scrapy) (19.10.0)\n",
      "Requirement already satisfied: pyOpenSSL in /opt/anaconda3/lib/python3.7/site-packages (from Scrapy) (19.0.0)\n",
      "Requirement already satisfied: queuelib in /opt/anaconda3/lib/python3.7/site-packages (from Scrapy) (1.5.0)\n",
      "Requirement already satisfied: service-identity in /opt/anaconda3/lib/python3.7/site-packages (from Scrapy) (18.1.0)\n",
      "Requirement already satisfied: w3lib>=1.17.0 in /opt/anaconda3/lib/python3.7/site-packages (from Scrapy) (1.21.0)\n",
      "Requirement already satisfied: cssselect>=0.9 in /opt/anaconda3/lib/python3.7/site-packages (from Scrapy) (1.1.0)\n",
      "Requirement already satisfied: lxml in /opt/anaconda3/lib/python3.7/site-packages (from Scrapy) (4.4.1)\n",
      "Requirement already satisfied: PyDispatcher>=2.0.5 in /opt/anaconda3/lib/python3.7/site-packages (from Scrapy) (2.0.5)\n",
      "Requirement already satisfied: six>=1.5.2 in /opt/anaconda3/lib/python3.7/site-packages (from Scrapy) (1.12.0)\n",
      "Requirement already satisfied: parsel>=1.5 in /opt/anaconda3/lib/python3.7/site-packages (from Scrapy) (1.5.2)\n",
      "Requirement already satisfied: zope.interface>=4.4.2 in /opt/anaconda3/lib/python3.7/site-packages (from Twisted>=13.1.0->Scrapy) (4.7.1)\n",
      "Requirement already satisfied: incremental>=16.10.1 in /opt/anaconda3/lib/python3.7/site-packages (from Twisted>=13.1.0->Scrapy) (17.5.0)\n",
      "Requirement already satisfied: constantly>=15.1 in /opt/anaconda3/lib/python3.7/site-packages (from Twisted>=13.1.0->Scrapy) (15.1.0)\n",
      "Requirement already satisfied: attrs>=17.4.0 in /opt/anaconda3/lib/python3.7/site-packages (from Twisted>=13.1.0->Scrapy) (19.2.0)\n",
      "Requirement already satisfied: PyHamcrest>=1.9.0 in /opt/anaconda3/lib/python3.7/site-packages (from Twisted>=13.1.0->Scrapy) (1.9.0)\n",
      "Requirement already satisfied: Automat>=0.3.0 in /opt/anaconda3/lib/python3.7/site-packages (from Twisted>=13.1.0->Scrapy) (0.8.0)\n",
      "Requirement already satisfied: hyperlink>=17.1.1 in /opt/anaconda3/lib/python3.7/site-packages (from Twisted>=13.1.0->Scrapy) (19.0.0)\n",
      "Requirement already satisfied: cryptography>=2.3 in /opt/anaconda3/lib/python3.7/site-packages (from pyOpenSSL->Scrapy) (2.7)\n",
      "Requirement already satisfied: pyasn1-modules in /opt/anaconda3/lib/python3.7/site-packages (from service-identity->Scrapy) (0.2.7)\n",
      "Requirement already satisfied: pyasn1 in /opt/anaconda3/lib/python3.7/site-packages (from service-identity->Scrapy) (0.4.8)\n",
      "Requirement already satisfied: setuptools in /opt/anaconda3/lib/python3.7/site-packages (from zope.interface>=4.4.2->Twisted>=13.1.0->Scrapy) (41.4.0)\n",
      "Requirement already satisfied: idna>=2.5 in /opt/anaconda3/lib/python3.7/site-packages (from hyperlink>=17.1.1->Twisted>=13.1.0->Scrapy) (2.8)\n",
      "Requirement already satisfied: asn1crypto>=0.21.0 in /opt/anaconda3/lib/python3.7/site-packages (from cryptography>=2.3->pyOpenSSL->Scrapy) (1.0.1)\n",
      "Requirement already satisfied: cffi!=1.11.3,>=1.8 in /opt/anaconda3/lib/python3.7/site-packages (from cryptography>=2.3->pyOpenSSL->Scrapy) (1.12.3)\n",
      "Requirement already satisfied: pycparser in /opt/anaconda3/lib/python3.7/site-packages (from cffi!=1.11.3,>=1.8->cryptography>=2.3->pyOpenSSL->Scrapy) (2.19)\n"
     ]
    }
   ],
   "source": [
    "!pip install Scrapy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import csv\n",
    "import scrapy\n",
    "import time\n",
    "from datetime import date\n",
    "import re\n",
    "\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "# allows you to search for things using specific parameters\n",
    "from selenium.webdriver.common.by import By\n",
    "# allows you to wait for a page to load\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "# tells you locator is unable to find the web element \n",
    "from selenium.common.exceptions import NoSuchElementException\n",
    "# Handling a timeout situation\n",
    "from selenium.common.exceptions import TimeoutException\n",
    "# specify what you are looking for on a specific page in order to \n",
    "# determine that the webpage has loaded\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "# way of automating low level interactions like mouse movements\n",
    "# mouse button actions, key presses and context menu interactions\n",
    "from selenium.webdriver.common.action_chains import ActionChains\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.common.exceptions import NoSuchElementException"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting webdriver-manager\n",
      "  Downloading https://files.pythonhosted.org/packages/c2/f6/bd9d73991fd8d1b488aefa4b55711af670e938a9b8549ffd8ecc42780069/webdriver_manager-2.3.0-py2.py3-none-any.whl\n",
      "Collecting crayons (from webdriver-manager)\n",
      "  Downloading https://files.pythonhosted.org/packages/f8/64/ab71c69db049a5f404f1f2c7627578f4b59aca55e6ad9d939721ce6466dd/crayons-0.3.0-py2.py3-none-any.whl\n",
      "Requirement already satisfied: requests in /opt/anaconda3/lib/python3.7/site-packages (from webdriver-manager) (2.22.0)\n",
      "Collecting configparser (from webdriver-manager)\n",
      "  Downloading https://files.pythonhosted.org/packages/7a/2a/95ed0501cf5d8709490b1d3a3f9b5cf340da6c433f896bbe9ce08dbe6785/configparser-4.0.2-py2.py3-none-any.whl\n",
      "Requirement already satisfied: colorama in /opt/anaconda3/lib/python3.7/site-packages (from crayons->webdriver-manager) (0.4.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/anaconda3/lib/python3.7/site-packages (from requests->webdriver-manager) (2019.9.11)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /opt/anaconda3/lib/python3.7/site-packages (from requests->webdriver-manager) (1.24.2)\n",
      "Requirement already satisfied: idna<2.9,>=2.5 in /opt/anaconda3/lib/python3.7/site-packages (from requests->webdriver-manager) (2.8)\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /opt/anaconda3/lib/python3.7/site-packages (from requests->webdriver-manager) (3.0.4)\n",
      "Installing collected packages: crayons, configparser, webdriver-manager\n",
      "Successfully installed configparser-4.0.2 crayons-0.3.0 webdriver-manager-2.3.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install webdriver-manager"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.common.exceptions import NoSuchElementException\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "\n",
    "driver = webdriver.Chrome(ChromeDriverManager().install())\n",
    "driver.get(\"https://www.sephora.com\")\n",
    "\n",
    "#chrome_path = \"/Users/radhevenkatesan/Desktop/Data Science Projects/chromedriver\"\n",
    "\n",
    "def scrollDown(driver, n_scroll):\n",
    "    body = driver.find_element_by_tag_name(\"body\")\n",
    "    while n_scroll >= 0:\n",
    "        body.send_keys(Keys.PAGE_DOWN)\n",
    "        n_scroll -= 1\n",
    "    return driver\n",
    "\n",
    "xpath = '/html/body/div[5]/div/div/div[1]/div/div/button'\n",
    "btn = driver.find_element_by_xpath(xpath)\n",
    "btn.click()\n",
    "xpath2 = '/html/body/div[3]/div/div/div[1]/div/div/div[2]/form/div[3]/div/div[1]/button'\n",
    "btn = driver.find_element_by_xpath(xpath2)\n",
    "btn.click()\n",
    "\n",
    "# initiate empty dataframe\n",
    "df = pd.DataFrame(columns=['Label', 'URL'])\n",
    "print(df)\n",
    "\n",
    "# step 1\n",
    "tickers = ['moisturizing-cream-oils-mists', 'cleanser', 'facial-treatments', 'face-mask',\n",
    "           'eye-treatment-dark-circle-treatment', 'sunscreen-sun-protection']\n",
    "\n",
    "for ticker in tickers:\n",
    "    url = 'https://www.sephora.com/shop/' + ticker + '?pageSize=300'\n",
    "    driver.get(url)\n",
    "\n",
    "    xpath = '/html/body/div[5]/div/div/div[1]/div/div/button'\n",
    "    btn = driver.find_element_by_xpath(xpath)\n",
    "    btn.click()\n",
    "    time.sleep(20)\n",
    "\n",
    "    browser = scrollDown(driver, 10)\n",
    "    time.sleep(10)\n",
    "\n",
    "    browser = scrollDown(driver, 10)\n",
    "    time.sleep(10)\n",
    "\n",
    "    browser = scrollDown(driver, 10)\n",
    "    time.sleep(10)\n",
    "\n",
    "    browser = scrollDown(driver, 10)\n",
    "\n",
    "    element = driver.find_elements_by_class_name('css-ix8km1')\n",
    "\n",
    "    subpageURL = []\n",
    "    for a in element:\n",
    "        subURL = a.get_attribute('href')\n",
    "        subpageURL.append(subURL)\n",
    "\n",
    "    # transform into a data frame\n",
    "    dic = {'Label': ticker, 'URL': subpageURL}\n",
    "    df = df.append(pd.DataFrame(dic), ignore_index = True)\n",
    "\n",
    "# add columns\n",
    "df2 = pd.DataFrame(columns=['brand', 'name', 'price', 'rank', 'skin_type', 'ingredients'])\n",
    "df = pd.concat([df, df2], axis = 1)\n",
    "\n",
    "# step 2\n",
    "for i in range(len(df)+1):\n",
    "    url = df.URL[i]\n",
    "    driver.get(url)\n",
    "    time.sleep(5)\n",
    "\n",
    "    xpath = '/html/body/div[5]/div/div/div[1]/div/div/button'\n",
    "    btn = driver.find_element_by_xpath(xpath)\n",
    "    btn.click()\n",
    "\n",
    "    # brand, name, price\n",
    "    df.brand[i] = driver.find_element_by_class_name('css-avdj50').text\n",
    "    df.name[i] = driver.find_element_by_class_name('css-r4ddnb ').text\n",
    "    df.price[i] = driver.find_element_by_class_name('css-n8yjg7 ').text\n",
    "\n",
    "    browser = scrollDown(driver, 1)\n",
    "    time.sleep(5)\n",
    "    browser = scrollDown(driver, 1)\n",
    "    time.sleep(5)\n",
    "\n",
    "    # skin_type\n",
    "    detail = driver.find_element_by_class_name('css-192qj50').text\n",
    "    pattern = r\"✔ \\w+\\n\"\n",
    "    df.skin_type[i] = re.findall(pattern, detail)\n",
    "\n",
    "    # ingredients\n",
    "    xpath = '//*[@id=\"tab2\"]'\n",
    "    btn = driver.find_element_by_xpath(xpath)\n",
    "    btn.click()\n",
    "\n",
    "    try:\n",
    "        df.ingredients[i] = driver.find_element_by_xpath('//*[@id=\"tabpanel2\"]/div').text\n",
    "    except NoSuchElementException:\n",
    "        df.ingredients[i] = 'No Info'\n",
    "\n",
    "    # rank\n",
    "    try:\n",
    "        rank = driver.find_element_by_class_name('css-ffj77u').text\n",
    "        rank = re.match('\\d.\\d', rank).group()\n",
    "        df['rank'][i] = str(rank)\n",
    "\n",
    "    except NoSuchElementException:\n",
    "        df['rank'][i] = 0\n",
    "\n",
    "    print(i)    # just for verbose\n",
    "\n",
    "\n",
    "df.to_csv('data/cosmetic.csv', encoding = 'utf-8-sig', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
